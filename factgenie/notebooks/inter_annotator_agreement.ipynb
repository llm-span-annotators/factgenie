{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pygamma-agreement cylp pandas numpy tqdm scipy matplotlib ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing inter-annotator agreement (IAA) with factgenie\n",
    "\n",
    "This notebook shows how to compute inter-annotator agreement (IAA) between annotators.\n",
    "\n",
    "# Pearson r\n",
    "\n",
    "First, we will use the **Pearson correlation coefficient** to measure the agreement between two annotator groups.\n",
    "\n",
    "Specifically, we will measure how much the **error counts** agree. \n",
    "\n",
    "In the ideal case, both annotator groups annotated the **same amount of errors of each category** for each example. The Pearson r coefficient will help us to quantify to which extent it is true. The value of 1 signifies perfect *positive linear correlation*, 0 signifies no linear correlation, -1 signifies perfect *negative linear correllation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "\n",
    "def compute_pearson_r(df, group1, group2):\n",
    "    group1_data = df[df['annotator_group_id'] == group1]\n",
    "    group2_data = df[df['annotator_group_id'] == group2]\n",
    "    \n",
    "    group1_counts = list(group1_data[\"count\"])\n",
    "    group2_counts = list(group2_data[\"count\"])\n",
    "\n",
    "    # Micro correlation - correlation of counts\n",
    "    micro_corr = pearsonr(group1_counts, group2_counts)[0]\n",
    "\n",
    "    # Macro correlation - average of per-type correlations\n",
    "    type_corrs = []\n",
    "    for ann_type in df['annotation_type'].unique():\n",
    "        g1_type = list(group1_data[group1_data['annotation_type'] == ann_type][\"count\"])\n",
    "        g2_type = list(group2_data[group2_data['annotation_type'] == ann_type][\"count\"])\n",
    "\n",
    "        type_corrs.append(pearsonr(g1_type, g2_type)[0])\n",
    "    \n",
    "    macro_corr = np.mean(type_corrs)\n",
    "    \n",
    "    return {'micro': micro_corr, 'macro': macro_corr, 'category_correlations': type_corrs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Input data\n",
    "For computing Pearson r, you will need (at least one of) the CSV files generated by factgenie:\n",
    "- `example_level_counts.csv` - absolute error counts for each (dataset, split, setup_id, example_idx) combination,\n",
    "- `dataset_level_counts.csv` - average error counts for each (dataset, split, setup_id) combination.\n",
    "\n",
    "You can generate these files on the `/analyze` page (on the Inter-annotator agreement tab). On that page, you need to select the campaign(s) with multiple annotators per example and select `Export data files`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory where the csv files are located here\n",
    "csv_dir = \".\"\n",
    "\n",
    "level = \"example\"\n",
    "\n",
    "csv_filename = f\"{csv_dir}/{level}_level_counts.csv\"\n",
    "# Load data\n",
    "df = pd.read_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotator groups\n",
    "We will always compute the correlation between two **annotator groups**. Each annotator group has an id in the format `{campaign_id}-anngroup-{group_idx}`. That means that it uniquely defines the ordinal number of the annotator within a specific campaign.\n",
    "\n",
    "Example: in the campaign `llm-eval-1`, you used two annotators per example. Then you want to measure agreement between `llm-eval-1-anngroup-0` and `llm-eval-1-anngroup-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all annotator groups\n",
    "annotator_groups = df.annotator_group_id.unique()\n",
    "\n",
    "print(f\"Groups: {annotator_groups}\")\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "group_pairs = list(combinations(annotator_groups, 2))\n",
    "# or use a list of tuples: [(\"group1\", \"group2\"), ...]\n",
    "\n",
    "group_pairs = [(\"highlights-b-longer-anngroup-30\", \"highlights-a-longer-anngroup-30\")]\n",
    "\n",
    "print(f\"Group pairs: {group_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Average type\n",
    "- **Micro-average** - a coefficient computed over concatenated results from all the categories.\n",
    "- **Macro-average** - an average of coefficients computed separately for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations for all pairs of groups\n",
    "for group1, group2 in group_pairs:\n",
    "    correlations = compute_pearson_r(df, group1, group2)\n",
    "\n",
    "    print(f\"{level}-level correlations between {group1} and {group2}\")\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    print(f\"Micro Pearson-r: {correlations['micro']:.3f}\")\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    for i, corr in enumerate(correlations['category_correlations']):\n",
    "        print(f\"Category {i}: {corr:.3f}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(f\"Macro Pearson-r: {correlations['macro']:.3f}\")\n",
    "    print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma (γ) score\n",
    "Next, we compute the gamma (γ) score.\n",
    "\n",
    "This score suitable for computing IAA in cases where are both (1) determining span positions and (2) categorizing the spans.\n",
    "\n",
    "The γ score considers the best alignment between the spans and computes the value based on the number of local dissimilarities. The score will help us to quantify the correlation not just between the error counts, but also their exact **positions** on top of the output text.\n",
    "\n",
    "For full description, please refer to the original paper [Mathet et al. (2015)](https://doi.org/10.1162/COLI_a_00227).\n",
    "\n",
    "For Python, the score is implemented in the [pygamma-agreement](https://pygamma-agreement.readthedocs.io/en/latest/index.html) library.\n",
    "\n",
    "**Note that computing the score is computationally intensive. Consider saving intermediate per-example scores in case you need to repeat the experiments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import pygamma_agreement as pa\n",
    "from pyannote.core import Segment\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def compute_gamma(span_index, dissim):\n",
    "    gamma_scores = []\n",
    "    running_avg = 0\n",
    "    \n",
    "    # Group examples\n",
    "    groups = list(span_index.groupby([\"dataset\", \"split\", \"setup_id\", \"example_idx\"]))\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=len(groups), desc='Computing gamma score')\n",
    "    \n",
    "\n",
    "    display.display(f\"Visualization of the best alignment\", display_id='vis_lbl')\n",
    "    display.display(f\"Waiting for the first example...\", display_id='vis')\n",
    "\n",
    "    for idx, (i, group) in enumerate(groups, 1):\n",
    "        try:\n",
    "            # Add each annotation to continuum\n",
    "            continuum = pa.Continuum()\n",
    "\n",
    "            if group.annotator_group_id.unique().shape[0] < 2:\n",
    "                # One of the annotators did not add any annotation -> gamma = 0\n",
    "                print(f\"Example {idx} has annotations from only one annotator group\")\n",
    "                gamma_scores.append(0.0)\n",
    "                running_avg = np.mean(gamma_scores)\n",
    "                pbar.set_postfix({'avg_gamma': f'{running_avg:.3f}'})\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            for j, row in group.iterrows():\n",
    "                # make sure we do not add empty segments\n",
    "                if row[\"annotation_start\"] == row[\"annotation_end\"]:\n",
    "                    continue\n",
    "\n",
    "                continuum.add(\n",
    "                    str(row[\"annotator_group_id\"]),\n",
    "                    Segment(row[\"annotation_start\"], row[\"annotation_end\"]),\n",
    "                    str(row[\"annotation_type\"]),\n",
    "                )\n",
    "\n",
    "            # Temporarily increase logging level to suppress output\n",
    "            logging.getLogger().setLevel(logging.WARNING)\n",
    "            gamma_results = continuum.compute_gamma(dissim, soft=True)\n",
    "            logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "            # Show the best alignment for every 10th example\n",
    "            if pbar.n % 10 == 0:\n",
    "                display.update_display(f\"Visualization of the best alignment for example {idx}\", display_id='vis_lbl')\n",
    "                display.update_display(gamma_results.best_alignment, display_id='vis')\n",
    "\n",
    "            gamma_scores.append(gamma_results.gamma)\n",
    "            running_avg = np.mean(gamma_scores)\n",
    "            \n",
    "            # Update progress bar with current average\n",
    "            pbar.set_postfix({'avg_gamma': f'{running_avg:.3f}'})\n",
    "            pbar.update(1)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"Error computing gamma for example {idx}\")\n",
    "            gamma_scores.append(0.0)\n",
    "            running_avg = np.mean(gamma_scores)\n",
    "            pbar.set_postfix({'avg_gamma': f'{running_avg:.3f}'})\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    return float(np.mean(gamma_scores)) if gamma_scores else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_spans = pd.read_csv(f\"{csv_dir}/gamma_spans.csv\")\n",
    "\n",
    "\n",
    "annotator_groups = gamma_spans.annotator_group_id.unique()\n",
    "# you can define the list of relevant groups (2 or more) here\n",
    "# groups = [\"group1\", \"group2\", ...]\n",
    "annotator_groups = ['highlights-a-longer-anngroup-12', 'highlights-b-longer-anngroup-12']\n",
    "print(f\"Groups: {annotator_groups}\")\n",
    "\n",
    "if len(annotator_groups) > 5:\n",
    "    print(\"==============================================\")\n",
    "    print(f\"WARNING: more than 5 groups detected, computation may be slow\")\n",
    "\n",
    "gamma_spans = gamma_spans[gamma_spans[\"annotator_group_id\"].isin(annotator_groups)]\n",
    "\n",
    "\n",
    "# `alpha`: coefficient weighting the *positional* dissimilarity value, defaults to 1\n",
    "alpha = 1\n",
    "# `beta`: coefficient weighting the *categorical* dissimilarity value, defaults to 1\n",
    "beta = 1\n",
    "# `delta_empty`: empty dissimilarity value, defaults to 1\n",
    "dissim = pa.CombinedCategoricalDissimilarity(delta_empty=1, alpha=1, beta=1)\n",
    "gamma = compute_gamma(gamma_spans, dissim)\n",
    "\n",
    "print(f\"==============================================\")\n",
    "print(f\"Gamma score: {gamma:.3f}\")\n",
    "print(f\"==============================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
