{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pygamma-agreement cylp pandas numpy tqdm scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing inter-annotator agreement (IAA) with factgenie\n",
    "\n",
    "This notebook shows how to compute inter-annotator agreement (IAA) between two annotator groups.\n",
    "\n",
    "### Input data\n",
    "For using the notebook, you will need the CSV files generated by factgenie for computing inter-annotator agreement:\n",
    "- `dataset_level_counts.csv`\n",
    "- `example_level_counts.csv`\n",
    "- `gamma_spans.csv`\n",
    "\n",
    "You can generate these files on the `/analyze` page (on the Inter-annotator agreement tab). On that page, you need to select the campaign(s) with multiple annotators per example and select `Export data files`.\n",
    "\n",
    "### Annotator groups\n",
    "We will compute the correlation between two **annotator groups**. Each annotator group has an id in the format `{campaign_id}-anngroup-{group_idx}`. That means that it uniquely defines the ordinal number of the annotator within a specific campaign.\n",
    "\n",
    "#### Single campaign\n",
    "You can compute IAA between annotators within a single campaign.\n",
    "\n",
    "Example: in the campaign `llm-eval-1`, you used two annotators per example. Then you want to measure agreement between `llm-eval-1-anngroup-0` and `llm-eval-1-anngroup-1`.\n",
    "\n",
    "#### Multiple campaigns\n",
    "You can compute IAA between annotators in multiple campaigns **if these campaigns were annotating the same outputs**.\n",
    "\n",
    "Example: you ran campaigns `llm-eval-1` and `llm-eval-2` over the same set of examples. Then you will measure agreement between `llm-eval-1-anngroup-0` and `llm-eval-2-anngroup-0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import pygamma_agreement as pa\n",
    "import traceback\n",
    "from pyannote.core import Segment\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Set the directory where the csv files are located here\n",
    "csv_path = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson r\n",
    "\n",
    "First, we will use the **Pearson correlation coefficient** to measure the agreement between two annotators.\n",
    "\n",
    "Specifically, we will measure how much the **error counts** agree. \n",
    "\n",
    "In the ideal case, both annotators annotated the **same amount of errors of each category** for each example. The Pearson r coefficient will help us to quantify to which extent it is true. The value of 1 signifies perfect *positive linear correlation*, 0 signifies no linear correlation, -1 signifies perfect *negative linear correllation*.\n",
    "\n",
    "We will compare both the example-level correlation, which is more strict, and dataset-level (or, more precisely, dataset-split-setup_id-level) correlation, which is more lenient.\n",
    "\n",
    "## Levels\n",
    "\n",
    "### Dataset-level\n",
    "Pearson r between two annotators computed over a list of average error counts for each (dataset, split, setup_id) combination.\n",
    "\n",
    "### Example-level\n",
    "Pearson r between two annotators computed over a list of error counts for each (dataset, split, setup_id, example_idx) combination.\n",
    "\n",
    "## Average type\n",
    "\n",
    "### Micro-average\n",
    "A coefficient computed over concatenated results from all the categories.\n",
    "\n",
    "### Macro-average\n",
    "An average of coefficients computed separately for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson_r(csv_path, group1, group2):\n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    group1_data = df[df['annotator_group_id'] == group1]\n",
    "    group2_data = df[df['annotator_group_id'] == group2]\n",
    "    \n",
    "    group1_counts = list(group1_data[\"count\"])\n",
    "    group2_counts = list(group2_data[\"count\"])\n",
    "\n",
    "    # Micro correlation - correlation of counts\n",
    "    micro_corr = pearsonr(group1_counts, group2_counts)[0]\n",
    "\n",
    "    # Macro correlation - average of per-type correlations\n",
    "    type_corrs = []\n",
    "    for ann_type in df['annotation_type'].unique():\n",
    "        g1_type = list(group1_data[group1_data['annotation_type'] == ann_type][\"count\"])\n",
    "        g2_type = list(group2_data[group2_data['annotation_type'] == ann_type][\"count\"])\n",
    "\n",
    "        type_corrs.append(pearsonr(g1_type, g2_type)[0])\n",
    "    \n",
    "    macro_corr = np.mean(type_corrs)\n",
    "    \n",
    "    return {'micro': micro_corr, 'macro': macro_corr, 'category_correlations': type_corrs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for level in [\"dataset\", \"example\"]:\n",
    "    csv_filename = f\"{csv_path}/{level}_level_counts.csv\"\n",
    "\n",
    "    groups = ('quintd1-gpt-4-anngroup-0', 'quintd1-human-anngroup-0')\n",
    "    correlations = compute_pearson_r(csv_filename, *groups)\n",
    "\n",
    "    print(f\"{level}-level correlations between {groups[0]} and {groups[1]}\")\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    print(f\"Micro Pearson-r: {correlations['micro']:.3f}\")\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    for i, corr in enumerate(correlations['category_correlations']):\n",
    "        print(f\"Category {i}: {corr:.3f}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    print(f\"Macro Pearson-r: {correlations['macro']:.3f}\")\n",
    "    print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma (γ) score\n",
    "Second, we compute the gamma (γ) score between the two annotator groups.\n",
    "\n",
    "This score suitable for computing IAA in cases where are both (1) determining span positions and (2) categorizing the spans.\n",
    "\n",
    "The γ score considers the best alignment between the spans and computes the value based on the number of local dissimilarities. The score will help us to quantify the correlation not just between the error counts, but also their exact **positions** on top of the output text.\n",
    "\n",
    "For full description, please refer to the original paper [Mathet et al. (2015)](https://doi.org/10.1162/COLI_a_00227).\n",
    "\n",
    "For Python, the score is implemented in the [pygamma-agreement](https://pygamma-agreement.readthedocs.io/en/latest/index.html) library.\n",
    "\n",
    "**Note that computing the score is computationally intensive. Consider saving intermediate per-example scores in case you need to repeat the experiments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def compute_gamma(span_index, dissim):\n",
    "    gamma_scores = []\n",
    "    running_avg = 0\n",
    "    \n",
    "    # Group examples\n",
    "    groups = list(span_index.groupby([\"dataset\", \"split\", \"setup_id\", \"example_idx\"]))\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=len(groups), desc='Computing gamma score')\n",
    "    \n",
    "\n",
    "    display.display(f\"Visualization of the best alignment\", display_id='vis_lbl')\n",
    "    display.display(f\"Waiting for the first example...\", display_id='vis')\n",
    "    \n",
    "    for idx, (i, group) in enumerate(groups, 1):\n",
    "        try:\n",
    "            # Add each annotation to continuum\n",
    "            continuum = pa.Continuum()\n",
    "\n",
    "            if group.annotator_group_id.unique().shape[0] < 2:\n",
    "                # One of the annotators did not add any annotation -> gamma = 0\n",
    "                gamma_scores.append(0.0)\n",
    "                running_avg = np.mean(gamma_scores)\n",
    "                pbar.set_postfix({'avg_gamma': f'{running_avg:.3f}'})\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            for j, row in group.iterrows():\n",
    "                # make sure we do not add empty segments\n",
    "                if row[\"annotation_start\"] == row[\"annotation_end\"]:\n",
    "                    continue\n",
    "\n",
    "                continuum.add(\n",
    "                    str(row[\"annotator_group_id\"]),\n",
    "                    Segment(row[\"annotation_start\"], row[\"annotation_end\"]),\n",
    "                    str(row[\"annotation_type\"]),\n",
    "                )\n",
    "\n",
    "            # Temporarily increase logging level to suppress output\n",
    "            logging.getLogger().setLevel(logging.WARNING)\n",
    "            gamma_results = continuum.compute_gamma(dissim, soft=True)\n",
    "            logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "            # Show the best alignment for every 10th example\n",
    "            if idx % 10 == 1:\n",
    "                display.update_display(f\"Visualization of the best alignment for example {idx}\", display_id='vis_lbl')\n",
    "                display.update_display(gamma_results.best_alignment, display_id='vis')\n",
    "\n",
    "            gamma_scores.append(gamma_results.gamma)\n",
    "            running_avg = np.mean(gamma_scores)\n",
    "            \n",
    "            # Update progress bar with current average\n",
    "            pbar.set_postfix({'avg_gamma': f'{running_avg:.3f}'})\n",
    "            pbar.update(1)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"Error computing gamma for example {idx}\")\n",
    "            gamma_scores.append(0.0)\n",
    "            running_avg = np.mean(gamma_scores)\n",
    "            pbar.set_postfix({'avg_gamma': f'{running_avg:.3f}'})\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    return float(np.mean(gamma_scores)) if gamma_scores else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_spans = pd.read_csv(f\"{csv_path}/gamma_spans.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `alpha`: coefficient weighting the *positional* dissimilarity value, defaults to 1\n",
    "alpha = 1\n",
    "# `beta`: coefficient weighting the *categorical* dissimilarity value, defaults to 1\n",
    "beta = 1\n",
    "# `delta_empty`: empty dissimilarity value, defaults to 1\n",
    "dissim = pa.CombinedCategoricalDissimilarity(delta_empty=1, alpha=1, beta=1)\n",
    "gamma = compute_gamma(gamma_spans, dissim)\n",
    "\n",
    "print(f\"Gamma score: {gamma:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
